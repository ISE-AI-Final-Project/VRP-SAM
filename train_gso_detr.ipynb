{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\" Visual Prompt Encoder training (validation) code \"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from common import utils\n",
    "from common.evaluation import Evaluator\n",
    "from common.logger import AverageMeter, Logger\n",
    "from data.dataset import FSSDataset\n",
    "from model.VRP_encoder_SEN import VRP_encoder_SEN, build_SEN\n",
    "from SAM2pred import SAM_pred\n",
    "\n",
    "import model.DETR.util.misc as detr_utils\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(datapath='.', benchmark='gso_detr', logpath='gso_train_detr', bsz=4, lr=0.0001, weight_decay=1e-06, epochs=1, nworker=8, seed=321, fold=0, condition='mask', use_ignore=True, local_rank=0, num_query=50, backbone='resnet50', nshot=14, load_weight='', sam_weight='/home/icetenny/senior-1/segment-anything/model/sam_vit_h_4b8939.pth', clip_max_norm=0.1, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the environment variables for distributed training\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12345'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'\n",
    "\n",
    "\n",
    "sys.argv = ['run',\n",
    "    '--datapath', '.',\n",
    "    '--logpath', 'gso_train_detr',\n",
    "    '--benchmark', 'gso_detr',\n",
    "    '--backbone', 'resnet50',\n",
    "    '--fold', '0',\n",
    "    '--condition', 'mask',\n",
    "    '--num_query', '50',\n",
    "    '--epochs', '1',\n",
    "    '--lr', '1e-4',\n",
    "    '--bsz', '4',\n",
    "    '--local_rank', '0',\n",
    "    # '--load_weight', \"checkpoints/gso_train1/best_model_ep14.ptrom\",\n",
    "    '--sam_weight', \"/home/icetenny/senior-1/segment-anything/model/sam_vit_h_4b8939.pth\"\n",
    "]\n",
    "\n",
    "\n",
    "# Arguments parsing\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Visual Prompt Encoder Pytorch Implementation\"\n",
    ")\n",
    "parser.add_argument(\"--datapath\", type=str, default=\"ice\")\n",
    "parser.add_argument(\n",
    "    \"--benchmark\",\n",
    "    type=str,\n",
    "    default=\"coco\",\n",
    "    choices=[\"pascal\", \"coco\", \"fss\", \"gso\", \"gso_detr\"],\n",
    ")\n",
    "parser.add_argument(\"--logpath\", type=str, default=\"\")\n",
    "parser.add_argument(\n",
    "    \"--bsz\", type=int, default=2\n",
    ")  # batch size = num_gpu * bsz default num_gpu = 4\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1e-6)\n",
    "parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--nworker\", type=int, default=8)\n",
    "parser.add_argument(\"--seed\", type=int, default=321)\n",
    "parser.add_argument(\"--fold\", type=int, default=0, choices=[0, 1, 2, 3])\n",
    "parser.add_argument(\n",
    "    \"--condition\",\n",
    "    type=str,\n",
    "    default=\"scribble\",\n",
    "    choices=[\"point\", \"scribble\", \"box\", \"mask\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_ignore\",\n",
    "    type=bool,\n",
    "    default=True,\n",
    "    help=\"Boundaries are not considered during pascal training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local_rank\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"number of cpu threads to use during batch generation\",\n",
    ")\n",
    "parser.add_argument(\"--num_query\", type=int, default=50)\n",
    "parser.add_argument(\n",
    "    \"--backbone\",\n",
    "    type=str,\n",
    "    default=\"resnet50\",\n",
    "    choices=[\"vgg16\", \"resnet50\", \"resnet101\"],\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--nshot\",\n",
    "    type=int,\n",
    "    default=14,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--load_weight\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--sam_weight\",\n",
    "    type=str,\n",
    "    default=\"/home/icetenny/senior-1/segment-anything/model/sam_vit_h_4b8939.pth\",\n",
    ")\n",
    "\n",
    "\n",
    "# DETR\n",
    "\n",
    "parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "# * Segmentation\n",
    "parser.add_argument('--masks', action='store_true',\n",
    "                    help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "# Loss\n",
    "parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                    help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "# * Matcher\n",
    "parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                    help=\"Class coefficient in the matching cost\")\n",
    "parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                    help=\"L1 box coefficient in the matching cost\")\n",
    "parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                    help=\"giou box coefficient in the matching cost\")\n",
    "\n",
    "\n",
    "# * Loss coefficients\n",
    "parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                    help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_nshot(\n",
    "#     args, epoch, model, dataloader, criterion, optimizer, scheduler, nshot=5\n",
    "# ):\n",
    "#     r\"\"\"Train VRP_encoder model\"\"\"\n",
    "\n",
    "#     # pdb.set_trace()\n",
    "#     utils.fix_randseed(args.seed + epoch)\n",
    "#     model.module.train_mode()\n",
    "#     criterion.train()\n",
    "\n",
    "#     # average_meter = AverageMeter(dataloader.dataset)\n",
    "\n",
    "#     for idx, batch in enumerate(dataloader):\n",
    "\n",
    "#         batch = utils.to_cuda(batch)\n",
    "#         # protos = model.module.forward_nshot(\n",
    "#         #     args.condition,\n",
    "#         #     batch[\"query_img\"],\n",
    "#         #     batch[\"support_imgs\"],\n",
    "#         #     batch[\"support_masks\"],\n",
    "#         #     training,\n",
    "#         #     nshot=nshot,\n",
    "#         # )\n",
    "\n",
    "#         outputs = model.module.forward(\n",
    "#             args.condition,\n",
    "#             batch[\"query_img\"],\n",
    "#             batch[\"support_imgs\"][:, 0],\n",
    "#             batch[\"support_masks\"][:, 0],\n",
    "#             training=True,\n",
    "#             # nshot=nshot,\n",
    "#         )\n",
    "\n",
    "#         loss_dict = criterion(outputs, targets)\n",
    "#         weight_dict = criterion.weight_dict\n",
    "#         losses = sum(\n",
    "#             loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict\n",
    "#         )\n",
    "\n",
    "#         # reduce losses over all GPUs for logging purposes\n",
    "#         loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "#         loss_dict_reduced_unscaled = {\n",
    "#             f\"{k}_unscaled\": v for k, v in loss_dict_reduced.items()\n",
    "#         }\n",
    "#         loss_dict_reduced_scaled = {\n",
    "#             k: v * weight_dict[k]\n",
    "#             for k, v in loss_dict_reduced.items()\n",
    "#             if k in weight_dict\n",
    "#         }\n",
    "#         losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "#         loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "#         if not math.isfinite(loss_value):\n",
    "#             print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "#             print(loss_dict_reduced)\n",
    "#             sys.exit(1)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         if max_norm > 0:\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         metric_logger.update(\n",
    "#             loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled\n",
    "#         )\n",
    "#         metric_logger.update(class_error=loss_dict_reduced[\"class_error\"])\n",
    "#         metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "#         loss = model.module.compute_objective(logit_mask, batch[\"query_mask\"])\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         # print(loss)\n",
    "\n",
    "#         area_inter, area_union = Evaluator.classify_prediction(\n",
    "#             pred_mask.squeeze(1), batch\n",
    "#         )\n",
    "#         average_meter.update(\n",
    "#             area_inter, area_union, batch[\"class_id\"], loss.detach().clone()\n",
    "#         )\n",
    "#         average_meter.write_process(idx, len(dataloader), epoch, write_batch_idx=200)\n",
    "\n",
    "#     average_meter.write_result(\"Training\" if training else \"Validation\", epoch)\n",
    "#     avg_loss = utils.mean(average_meter.loss_buf)\n",
    "#     miou, fb_iou = average_meter.compute_iou()\n",
    "\n",
    "#     return avg_loss, miou, fb_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nshot(\n",
    "    args, epoch, model, dataloader, criterion, optimizer, scheduler, nshot=5, max_norm=0\n",
    "):\n",
    "    r\"\"\"Train VRP_encoder model\"\"\"\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    utils.fix_randseed(args.seed + epoch)\n",
    "    model.module.train_mode()\n",
    "    criterion.train()\n",
    "\n",
    "    metric_logger = detr_utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', detr_utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('class_error', detr_utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "\n",
    "    # for idx, batch in enumerate(dataloader):\n",
    "\n",
    "    for batch in metric_logger.log_every(dataloader, print_freq, header):\n",
    "        print(\"Batch----------------------\")\n",
    "\n",
    "        batch = utils.to_cuda(batch)\n",
    "\n",
    "        outputs = model.module.forward(\n",
    "            args.condition,\n",
    "            batch[\"query_img\"],\n",
    "            batch[\"support_imgs\"][:, 0],\n",
    "            batch[\"support_masks\"][:, 0],\n",
    "            training=True,\n",
    "            # nshot=nshot,\n",
    "        )\n",
    "\n",
    "        print(batch['query_img'].shape)\n",
    "\n",
    "        print(outputs[\"pred_logits\"].shape, outputs[\"pred_boxes\"].shape)\n",
    "\n",
    "        targets = []\n",
    "\n",
    "        for bbox, label in zip(batch['bboxes'], batch['unique_obj_id']):\n",
    "            target = {}\n",
    "            target['boxes'] = torch.Tensor(bbox)\n",
    "            target[\"labels\"] = torch.zeros_like(torch.Tensor(label)).to(torch.int64)\n",
    "            targets.append(utils.to_cuda(target))\n",
    "\n",
    "        print(targets)\n",
    "\n",
    "\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(\n",
    "            loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict\n",
    "        )\n",
    "        print(\"Losses\", losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_unscaled = {\n",
    "            f\"{k}_unscaled\": v for k, v in loss_dict_reduced.items()\n",
    "        }\n",
    "        loss_dict_reduced_scaled = {\n",
    "            k: v * weight_dict[k]\n",
    "            for k, v in loss_dict_reduced.items()\n",
    "            if k in weight_dict\n",
    "        }\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(\n",
    "            loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled\n",
    "        )\n",
    "        metric_logger.update(class_error=loss_dict_reduced[\"class_error\"])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # loss = model.module.compute_objective(logit_mask, batch[\"query_mask\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print(loss)\n",
    "\n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "\n",
    "     # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ":=========== Few-shot Seg. with VRP-SAM ===========\n",
      "|             datapath: .                       \n",
      "|            benchmark: gso_detr                \n",
      "|              logpath: gso_train_detr          \n",
      "|                  bsz: 4                       \n",
      "|                   lr: 0.0001                  \n",
      "|         weight_decay: 1e-06                   \n",
      "|               epochs: 1                       \n",
      "|              nworker: 8                       \n",
      "|                 seed: 321                     \n",
      "|                 fold: 0                       \n",
      "|            condition: mask                    \n",
      "|           use_ignore: True                    \n",
      "|           local_rank: 0                       \n",
      "|            num_query: 50                      \n",
      "|             backbone: resnet50                \n",
      "|                nshot: 14                      \n",
      "|          load_weight:                         \n",
      "|           sam_weight: /home/icetenny/senior-1/segment-anything/model/sam_vit_h_4b8939.pth\n",
      "|        clip_max_norm: 0.1                     \n",
      "|                masks: False                   \n",
      "|             aux_loss: True                    \n",
      "|       set_cost_class: 1                       \n",
      "|        set_cost_bbox: 5                       \n",
      "|        set_cost_giou: 2                       \n",
      "|       mask_loss_coef: 1                       \n",
      "|       dice_loss_coef: 1                       \n",
      "|       bbox_loss_coef: 5                       \n",
      "|       giou_loss_coef: 2                       \n",
      "|             eos_coef: 0.1                     \n",
      ":==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda 1 Local Rank 0\n"
     ]
    }
   ],
   "source": [
    "dist.init_process_group(backend=\"nccl\")\n",
    "\n",
    "local_rank = dist.get_rank()\n",
    "print(\"Num cuda\", torch.cuda.device_count(), \"Local Rank\", local_rank)\n",
    "\n",
    "# local_rank = args.local_rank\n",
    "torch.cuda.set_device(local_rank)\n",
    "device = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "if utils.is_main_process():\n",
    "    Logger.initialize(args, training=True)\n",
    "utils.fix_randseed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backbone # param.: 23685367\n",
      "Learnable # param.: 1725190\n",
      "Total # param.: 25410557\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "model, criterion = build_SEN(args=args, device=device)\n",
    "\n",
    "\n",
    "if utils.is_main_process():\n",
    "    Logger.log_params(model)\n",
    "\n",
    "\n",
    "# Load Weight\n",
    "if args.load_weight != \"\":\n",
    "    if os.path.exists(args.load_weight):\n",
    "        model.load_state_dict(torch.load(args.load_weight, map_location=device))\n",
    "        print(f\"Model loaded from {args.load_weight}\")\n",
    "    else:\n",
    "        print(f\"No saved model found at {args.load_weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (trn) Images are : 10000\n",
      "Total (trn) Query are : 87095\n"
     ]
    }
   ],
   "source": [
    "# Dataset initialization\n",
    "FSSDataset.initialize(\n",
    "    img_size=512, datapath=args.datapath, use_original_imgsize=False\n",
    ")\n",
    "dataset, dataloader_trn = FSSDataset.build_dataloader(\n",
    "    args.benchmark, args.bsz, args.nworker, args.fold, \"trn\", shot=args.nshot\n",
    ")\n",
    "\n",
    "# dataloader_val = FSSDataset.build_dataloader(\n",
    "#     args.benchmark, args.bsz, args.nworker, args.fold, \"val\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.330556, 0.715741, 0.213889, 0.246296]], [[0.459028, 0.880556, 0.495833, 0.238889]], [[0.200694, 0.627778, 0.104167, 0.074074]], [[0.494444, 0.503704, 0.280556, 0.374074]]] [[9], [8], [12], [15]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.495833, 0.559259, 0.222222, 0.166667]], [[0.293056, 0.655556, 0.455556, 0.474074]], [[0.488889, 0.496296, 0.1, 0.162963]], [[0.258333, 0.281481, 0.433333, 0.562963]]] [[9], [3], [7], [1]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.416667, 0.167593, 0.188889, 0.335185]], [[0.439583, 0.623148, 0.579167, 0.735185]], [[0.509722, 0.831481, 0.230556, 0.337037]], [[0.209722, 0.530556, 0.313889, 0.538889]]] [[17], [3], [3], [10]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.209722, 0.412963, 0.163889, 0.396296]], [[0.664583, 0.584259, 0.279167, 0.390741]], [[0.748611, 0.139815, 0.283333, 0.27963]], [[0.827083, 0.49537, 0.318056, 0.357407]]] [[15], [7], [14], [12]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.115278, 0.851852, 0.230556, 0.296296]], [[0.855556, 0.778704, 0.136111, 0.168519]], [[0.4875, 0.123148, 0.202778, 0.246296]], [[0.290972, 0.759259, 0.259722, 0.481481]]] [[16], [17], [17], [2]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.5125, 0.60463, 0.144444, 0.090741]], [[0.467361, 0.244444, 0.176389, 0.140741]], [[0.88125, 0.203704, 0.2375, 0.407407]], [[0.680556, 0.57037, 0.202778, 0.203704]]] [[19], [11], [7], [10]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.497222, 0.501852, 0.063889, 0.1]], [[0.39375, 0.021296, 0.2375, 0.042593]], [[0.9125, 0.80463, 0.175, 0.205556]], [[0.305556, 0.905556, 0.236111, 0.188889]]] [[3], [13], [9], [9]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.582639, 0.136111, 0.201389, 0.272222]], [[0.33125, 0.521296, 0.059722, 0.07963]], [[0.229167, 0.627778, 0.191667, 0.07037]], [[0.289583, 0.235185, 0.268056, 0.47037]]] [[14], [3], [6], [3]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.542361, 0.59537, 0.1875, 0.198148]], [[0.670833, 0.871296, 0.447222, 0.257407]], [[0.55625, 0.29537, 0.215278, 0.235185]], [[0.190972, 0.312963, 0.381944, 0.318519]]] [[13], [9], [18], [9]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.070833, 0.147222, 0.141667, 0.294444]], [[0.399306, 0.618519, 0.209722, 0.151852]], [[0.236806, 0.857407, 0.098611, 0.285185]], [[0.65, 0.422222, 0.061111, 0.133333]]] [[12], [18], [18], [14]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.425694, 0.464815, 0.131944, 0.166667]], [[0.811806, 0.135185, 0.376389, 0.27037]], [[0.707639, 0.30463, 0.145833, 0.164815]], [[0.761806, 0.458333, 0.279167, 0.435185]]] [[4], [9], [7], [9]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.425, 0.183333, 0.244444, 0.322222]], [[0.4125, 0.451852, 0.247222, 0.196296]], [[0.416667, 0.776852, 0.122222, 0.253704]], [[0.272917, 0.247222, 0.545833, 0.494444]]] [[4], [7], [20], [3]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.477778, 0.509259, 0.4, 0.455556]], [[0.741667, 0.561111, 0.516667, 0.762963]], [[0.423611, 0.875, 0.555556, 0.25]], [[0.4875, 0.87963, 0.777778, 0.240741]]] [[15], [20], [3], [1]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.620139, 0.769444, 0.2875, 0.461111]], [[0.454167, 0.369444, 0.616667, 0.738889]], [[0.517361, 0.816667, 0.965278, 0.366667]], [[0.265972, 0.936111, 0.045833, 0.112963]]] [[2], [3], [8], [4]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.528472, 0.611111, 0.465278, 0.433333]], [[0.066667, 0.196296, 0.133333, 0.392593]], [[0.527778, 0.597222, 0.225, 0.216667]], [[0.485417, 0.517593, 0.431944, 0.509259]]] [[14], [3], [9], [20]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.357639, 0.755556, 0.493056, 0.488889]], [[0.361111, 0.49537, 0.197222, 0.224074]], [[0.264583, 0.375926, 0.301389, 0.451852]], [[0.877778, 0.708333, 0.244444, 0.298148]]] [[19], [10], [7], [15]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.370139, 0.238889, 0.395833, 0.477778]], [[0.629861, 0.316667, 0.126389, 0.1]], [[0.501389, 0.5, 0.258333, 0.203704]], [[0.498611, 0.472222, 0.45, 0.944444]]] [[2], [8], [2], [6]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.326389, 0.478704, 0.205556, 0.15]], [[0.244444, 0.846296, 0.438889, 0.307407]], [[0.39375, 0.37963, 0.023611, 0.259259]], [[0.540278, 0.85, 0.261111, 0.292593]]] [[15], [17], [8], [5]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.676389, 0.872222, 0.238889, 0.255556]], [[0.464583, 0.707407, 0.295833, 0.244444]], [[0.368056, 0.750926, 0.211111, 0.368519]], [[0.327778, 0.557407, 0.077778, 0.174074]]] [[15], [6], [2], [16]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.677778, 0.863889, 0.038889, 0.05]], [[0.285417, 0.262963, 0.120833, 0.255556]], [[0.259722, 0.412037, 0.519444, 0.535185], [0.89375, 0.325926, 0.2125, 0.355556]], [[0.590278, 0.641667, 0.141667, 0.357407]]] [[8], [10], [2, 8], [13]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.534028, 0.438889, 0.1375, 0.188889]], [[0.495833, 0.387037, 0.047222, 0.103704]], [[0.398611, 0.141667, 0.052778, 0.172222]], [[0.643056, 0.771296, 0.202778, 0.209259]]] [[8], [12], [9], [18]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.617361, 0.688889, 0.140278, 0.088889]], [[0.936806, 0.564815, 0.126389, 0.233333]], [[0.431944, 0.485185, 0.325, 0.414815]], [[0.827778, 0.585185, 0.344444, 0.82963]]] [[18], [4], [10], [2]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.449306, 0.268519, 0.323611, 0.3]], [[0.227778, 0.860185, 0.438889, 0.27963]], [[0.5, 0.500926, 0.175, 0.353704]], [[0.425694, 0.774074, 0.106944, 0.111111]]] [[7], [6], [15], [17]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.325694, 0.219444, 0.548611, 0.438889]], [[0.779167, 0.173148, 0.252778, 0.27963]], [[0.593056, 0.225, 0.158333, 0.264815]], [[0.352778, 0.60463, 0.427778, 0.727778]]] [[1], [14], [6], [8]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.330556, 0.533333, 0.213889, 0.322222]], [[0.502083, 0.24537, 0.284722, 0.35]], [[0.572222, 0.197222, 0.136111, 0.190741]], [[0.130556, 0.402778, 0.261111, 0.331481]]] [[7], [9], [10], [3]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.641667, 0.207407, 0.288889, 0.307407]], [[0.417361, 0.180556, 0.5625, 0.361111]], [[0.634028, 0.633333, 0.176389, 0.277778]], [[0.630556, 0.477778, 0.238889, 0.414815]]] [[1], [20], [2], [6]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.924306, 0.628704, 0.151389, 0.272222]], [[0.890972, 0.301852, 0.218056, 0.248148]], [[0.560417, 0.447222, 0.598611, 0.894444]], [[0.404861, 0.739815, 0.309722, 0.190741]]] [[1], [3], [1], [20]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.501389, 0.497222, 0.269444, 0.364815]], [[0.507639, 0.501852, 0.056944, 0.162963]], [[0.2625, 0.410185, 0.2, 0.235185]], [[0.293056, 0.694444, 0.3, 0.411111]]] [[7], [1], [17], [3]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.253472, 0.630556, 0.343056, 0.301852]], [[0.270833, 0.606481, 0.447222, 0.57963]], [[0.497917, 0.853704, 0.165278, 0.292593]], [[0.252083, 0.562963, 0.1625, 0.188889]]] [[13], [9], [5], [19]]\n",
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 14, 512, 512]) torch.Size([4, 14, 3, 512, 512]) [[[0.504167, 0.497222, 0.344444, 0.131481]], [[0.915278, 0.480556, 0.169444, 0.346296]], [[0.429167, 0.650926, 0.4, 0.505556]], [[0.688194, 0.285185, 0.579167, 0.57037]]] [[5], [6], [17], [5]]\n"
     ]
    }
   ],
   "source": [
    "i=30\n",
    "for batch in dataloader_trn:\n",
    "    print(batch['query_img'].shape, batch['support_masks'].shape, batch['support_imgs'].shape, batch[\"bboxes\"], batch['unique_obj_id'])\n",
    "    i-=1\n",
    "\n",
    "    if i==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['query_img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": model.module.transformer_decoder.parameters()},\n",
    "        {\"params\": model.module.downsample_query.parameters(), \"lr\": args.lr},\n",
    "        {\"params\": model.module.merge_1.parameters(), \"lr\": args.lr},\n",
    "    ],\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "Evaluator.initialize(args)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=args.epochs * len(dataloader_trn)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 50, 256])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 50, 2]) torch.Size([4, 50, 4])\n",
      "[{'boxes': tensor([[0.3306, 0.7157, 0.2139, 0.2463]], device='cuda:0'), 'labels': tensor([0], device='cuda:0')}, {'boxes': tensor([[0.4590, 0.8806, 0.4958, 0.2389]], device='cuda:0'), 'labels': tensor([0], device='cuda:0')}, {'boxes': tensor([[0.2007, 0.6278, 0.1042, 0.0741]], device='cuda:0'), 'labels': tensor([0], device='cuda:0')}, {'boxes': tensor([[0.4944, 0.5037, 0.2806, 0.3741]], device='cuda:0'), 'labels': tensor([0], device='cuda:0')}]\n",
      "OWWO\n",
      "torch.Size([200, 2]) torch.Size([200, 4])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') torch.Size([4]) torch.int64\n",
      "tensor([[0.3306, 0.7157, 0.2139, 0.2463],\n",
      "        [0.4590, 0.8806, 0.4958, 0.2389],\n",
      "        [0.2007, 0.6278, 0.1042, 0.0741],\n",
      "        [0.4944, 0.5037, 0.2806, 0.3741]], device='cuda:0') torch.Size([4, 4]) torch.float32\n",
      "Indice [(tensor([24]), tensor([0])), (tensor([24]), tensor([0])), (tensor([35]), tensor([0])), (tensor([22]), tensor([0]))]\n",
      "Num_boxes 4\n",
      "torch.Size([100]) torch.Size([200])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') torch.Size([4]) torch.int64\n",
      "tensor([[0.3306, 0.7157, 0.2139, 0.2463],\n",
      "        [0.4590, 0.8806, 0.4958, 0.2389],\n",
      "        [0.2007, 0.6278, 0.1042, 0.0741],\n",
      "        [0.4944, 0.5037, 0.2806, 0.3741]], device='cuda:0') torch.Size([4, 4]) torch.float32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m----> 6\u001b[0m     trn_loss, trn_miou, trn_fb_iou \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_trn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#     val_loss, val_miou, val_fb_iou = train_nshot(\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#         args,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trn_miou \u001b[38;5;241m>\u001b[39m best_val_miou:\n",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m, in \u001b[0;36mtrain_nshot\u001b[0;34m(args, epoch, model, dataloader, criterion, optimizer, scheduler, nshot, max_norm)\u001b[0m\n\u001b[1;32m     42\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(utils\u001b[38;5;241m.\u001b[39mto_cuda(target))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(targets)\n\u001b[0;32m---> 47\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mweight_dict\n\u001b[1;32m     49\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m     50\u001b[0m     loss_dict[k] \u001b[38;5;241m*\u001b[39m weight_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m weight_dict\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sam6d/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/senior-1/VRP-SAM/model/DETR/criterion.py:209\u001b[0m, in \u001b[0;36mSetCriterion.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, aux_outputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 209\u001b[0m         indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43maux_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses:\n\u001b[1;32m    211\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    212\u001b[0m                 \u001b[38;5;66;03m# Intermediate masks losses are too costly to compute, we ignore them.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sam6d/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/sam6d/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/senior-1/VRP-SAM/model/DETR/matcher.py:79\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(tgt_bbox, tgt_bbox\u001b[38;5;241m.\u001b[39mshape, tgt_bbox\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m cost_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mout_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_ids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Compute the L1 cost between boxes\u001b[39;00m\n\u001b[1;32m     82\u001b[0m cost_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcdist(out_bbox, tgt_bbox, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "best_val_miou = float(\"-inf\")\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    trn_loss, trn_miou, trn_fb_iou = train_nshot(\n",
    "        args=args,\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        dataloader=dataloader_trn,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        nshot=args.nshot,\n",
    "        max_norm=args.clip_max_norm\n",
    "    )\n",
    "    # with torch.no_grad():\n",
    "    #     val_loss, val_miou, val_fb_iou = train_nshot(\n",
    "    #         args,\n",
    "    #         epoch,\n",
    "    #         model,\n",
    "    #         sam_model,\n",
    "    #         dataloader_val,\n",
    "    #         optimizer,\n",
    "    #         scheduler,\n",
    "    #         training=False,\n",
    "    #     )\n",
    "\n",
    "    # Save the best model\n",
    "    if trn_miou > best_val_miou:\n",
    "        best_val_miou = trn_miou\n",
    "        if utils.is_main_process():\n",
    "            Logger.save_model_miou(model, epoch, trn_miou)\n",
    "    if utils.is_main_process():\n",
    "        Logger.tbd_writer.add_scalars(\n",
    "            \"data/loss\", {\"trn_loss\": trn_loss, \"val_loss\": 0}, epoch\n",
    "        )\n",
    "        Logger.tbd_writer.add_scalars(\n",
    "            \"data/miou\", {\"trn_miou\": trn_miou, \"val_miou\": 0}, epoch\n",
    "        )\n",
    "        Logger.tbd_writer.add_scalars(\n",
    "            \"data/fb_iou\",\n",
    "            {\"trn_fb_iou\": trn_fb_iou, \"val_fb_iou\": 0},\n",
    "            epoch,\n",
    "        )\n",
    "        Logger.tbd_writer.flush()\n",
    "Logger.tbd_writer.close()\n",
    "Logger.info(\"==================== Finished Training ====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[0.3306, 0.7157, 0.2139, 0.2463]]), 'labels': tensor([9])},\n",
       " {'boxes': tensor([[0.4590, 0.8806, 0.4958, 0.2389]]), 'labels': tensor([8])},\n",
       " {'boxes': tensor([[0.2007, 0.6278, 0.1042, 0.0741]]), 'labels': tensor([12])},\n",
       " {'boxes': tensor([[0.4944, 0.5037, 0.2806, 0.3741]]), 'labels': tensor([15])}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = [{'boxes': [[0.330556, 0.715741, 0.213889, 0.246296]], 'labels': [9]}, {'boxes': [[0.459028, 0.880556, 0.495833, 0.238889]], 'labels': [8]}, {'boxes': [[0.200694, 0.627778, 0.104167, 0.074074]], 'labels': [12]}, {'boxes': [[0.494444, 0.503704, 0.280556, 0.374074]], 'labels': [15]}]\n",
    "\n",
    "for t in targets:\n",
    "    t['boxes'] = torch.tensor(t['boxes'])\n",
    "    t['labels'] = torch.tensor(t['labels'])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3306, 0.7157, 0.2139, 0.2463],\n",
       "        [0.4590, 0.8806, 0.4958, 0.2389],\n",
       "        [0.2007, 0.6278, 0.1042, 0.0741],\n",
       "        [0.4944, 0.5037, 0.2806, 0.3741]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "tgt_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  8, 12, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "tgt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2]), torch.Size([5]), torch.int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "out_prob = torch.tensor([\n",
    "    [0.1, 0.6],  # Probabilities for query 1\n",
    "    [0.2, 0.2],  # Probabilities for query 2\n",
    "    [0.7, 0.1],  # Probabilities for query 3\n",
    "    [0.1, 0.6],  # Probabilities for query 1\n",
    "    [0.2, 0.2],  # Probabilities for query 2\n",
    "    [0.7, 0.1],  # Probabilities for query 3\n",
    "])  # Shape: [3, 3] (3 queries, 3 classes)\n",
    "\n",
    "tgt_ids = torch.tensor([0, 0,0,0,0]).to(torch.int64)  # Ground truth classes for two targets\n",
    "\n",
    "out_prob.shape, tgt_ids.shape, tgt_ids.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1000, -0.1000, -0.1000, -0.1000, -0.1000],\n",
       "        [-0.2000, -0.2000, -0.2000, -0.2000, -0.2000],\n",
       "        [-0.7000, -0.7000, -0.7000, -0.7000, -0.7000],\n",
       "        [-0.1000, -0.1000, -0.1000, -0.1000, -0.1000],\n",
       "        [-0.2000, -0.2000, -0.2000, -0.2000, -0.2000],\n",
       "        [-0.7000, -0.7000, -0.7000, -0.7000, -0.7000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_class = -out_prob[:, tgt_ids]\n",
    "cost_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam6d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
